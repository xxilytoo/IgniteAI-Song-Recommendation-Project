# -*- coding: utf-8 -*-
"""NLPEmotionAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-X-rTndPptB6Quc4g8A5-atrzTlaDXfB
"""

import pandas as pd

df = pd.read_csv("tweet_emotions.csv")
df.head()

df = df.drop(columns=["tweet_id"])
df.head()

df.tail()

df.isnull().sum()

import re
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow import keras
from keras import layers, models

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight

from sklearn.metrics import classification_report
from sklearn.metrics import ConfusionMatrixDisplay

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

df["sentiment"].unique()

df = df[ (df.sentiment == 'happiness') | (df.sentiment == 'sadness') | (df.sentiment == 'love') | (df.sentiment == 'suprise') | (df.sentiment == 'worry')]
df["sentiment"] = df["sentiment"].astype("category")
df.label = df.sentiment.cat.remove_unused_categories()

sentiment_names = df.sentiment.cat.categories.tolist()

train_df, test_df = train_test_split(df, test_size=0.2, random_state=52)

print(f'{len(train_df)=}, {len(test_df)=}')
print(sentiment_names)

plt.figure(figsize=(12,3))
plt.bar(x = sentiment_names, height = np.bincount(df['sentiment'].cat.codes))

class_weights = dict(enumerate(
    compute_class_weight(
        class_weight="balanced",
        classes=pd.unique(df['sentiment']),
        y=df['sentiment']
    )
))

nltk.download('stopwords')

REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')    # add/remove regex as required
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
NUMBERS = re.compile('\d+')

STOPWORDS = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = tf.strings.lower(text)
    text = tf.strings.regex_replace(text, 'http\S+', '')
    text = tf.strings.regex_replace(text, '([@#][A-Za-z0-9_]+)|(\w+:\/\/\S+)', ' ')
    text = tf.strings.regex_replace(text, '[/(){}\[\]\|@,;]', ' ')
    text = tf.strings.regex_replace(text, '[^0-9a-z #+_]', '')
    text = tf.strings.regex_replace(text, '[\d+]', '')
    return text

def lemmatize_tokenize(text):
    # TODO: rework to use tf.strings
    # remove stopwords and lemmatize
    tokens = [word for word in text.split() if word not in STOPWORDS]
    tokens = [lemmatizer.lemmatize(token) for token in tokens]
    return tokens

N_CLASSES = len(sentiment_names)
MAX_FEATURES = 20000
MAX_SEQ_LEN = 1000
EMBEDDING_DIM = 100

vectorizer_layer = layers.TextVectorization(
    max_tokens=MAX_FEATURES,
    standardize=clean_text,
#     split=lemmatize_tokenize,
    output_sequence_length=MAX_SEQ_LEN,
    output_mode='int'
)
vectorizer_layer.adapt(train_df.content)

# Download and load pre-trained GloVe embeddings
#!wget http://nlp.stanford.edu/data/glove.6B.zip
#!unzip glove.6B.zip

# Load GloVe embeddings into memory
embeddings_index = {}
with open('glove.6B.100d.txt', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

# Get the vocabulary from the vectorizer layer
vocabulary = vectorizer_layer.get_vocabulary()

# Create a mapping from token to index
word_index = dict(zip(vocabulary, range(len(vocabulary))))

# Create embedding matrix
embedding_matrix = np.zeros((len(vocabulary), EMBEDDING_DIM))
for word, i in word_index.items():
    if i < MAX_FEATURES:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

model = models.Sequential([
    keras.Input(shape=(1,), dtype=tf.string),
    vectorizer_layer,
    layers.Embedding(MAX_FEATURES, EMBEDDING_DIM),

    layers.SpatialDropout1D(0.2),
    layers.GlobalMaxPooling1D(),
    layers.Dropout(0.4),
    layers.Dense(256, activation='gelu'),
    layers.Dropout(0.4),
    layers.Dense(N_CLASSES, activation='softmax'),
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

X_train, y_train = train_df.content, train_df.sentiment.cat.codes
X_test,  y_test  =  test_df.content,  test_df.sentiment.cat.codes

history = model.fit(
    x = X_train,
    y = y_train,
    validation_data=(X_test, y_test),
    batch_size=256,
    epochs=300,
    verbose=1,
    class_weight=class_weights,
    callbacks=[keras.callbacks.EarlyStopping(patience=3)],
)
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)

def plot_history(history):
    acc,  val_acc  = history['accuracy'],  history['val_accuracy']
    loss, val_loss = history['loss'], history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5)); plt.subplot(1, 2, 1)

    plt.plot(x, acc, 'b', label='Training acc')
    plt.plot(x, val_acc, 'r', label='Validation acc')
    plt.title('Training and validation accuracy'); plt.legend(); plt.subplot(1, 2, 2)

    plt.plot(x, loss, 'b', label='Training loss')
    plt.plot(x, val_loss, 'r', label='Validation loss')
    plt.title('Training and validation loss'); plt.legend(); plt.show()

plot_history(history.history)

y_pred = model.predict(X_test).argmax(1)

print(classification_report(
    y_test, y_pred, target_names= sentiment_names
))
ConfusionMatrixDisplay.from_predictions(
    y_test, y_pred, display_labels = sentiment_names
)